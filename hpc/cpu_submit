#!/bin/bash
#!
#! SLURM job script for Peta4-Skylake (Skylake CPUs, OPA)
#! Last updated: Mon 13 Nov 12:25:17 GMT 2017
#! https://docs.hpc.cam.ac.uk/hpc/user-guide/batch.html#sample-submission-scripts

#! Which project should be charged:
#SBATCH -A LEE-SL3-CPU

#! How many whole nodes should be allocated?
#! Note: Charging is determined by core number * walltime.
#SBATCH --nodes=1

#! How many (MPI) tasks will there be in total? (<= nodes*32)
#! The skylake/skylake-himem nodes have 32 CPUs (cores) each.
#! Each task is allocated 1 core by default, and each core is allocated 5990MB (skylake)
#! and 12040MB (skylake-himem). If this is insufficient, also specify
#! --cpus-per-task and/or --mem (the latter specifies MB per node).
#SBATCH --ntasks=1

#! What types of email messages to receive. Valid values include
#! NONE, BEGIN, END, FAIL, REQUEUE, ALL. See https://slurm.schedmd.com/sbatch.html.
#SBATCH --mail-type=END,FAIL

#! The partition to use
#! For 6GB per CPU, set "-p skylake"; for 12GB per CPU, set "-p skylake-himem":
#SBATCH -p skylake

#! ------ sbatch directives end here (put additional directives above this line)

#! Modify the settings below to specify the job's environment, location and launch method.
#! Optionally modify the environment seen by the application
#! (note that SLURM reproduces the environment at submission irrespective of ~/.bashrc):
. /etc/profile.d/modules.sh                # Leave this line (enables the module command)
module purge                               # Removes all modules still loaded
module load rhel7/default-peta4            # REQUIRED - loads the basic environment

#! --- Single Job ---
#! sbatch -J job_name -t 1:0:0 --export CMD='python path/to/script.py --cli-arg 42' hpc/cpu_submit

# --- Array Job ---
#! sbatch -J job_name -t 1:0:0 --array 0-15 --export CMD="python path/to/script.py --cli-arg 42 --random-seed \$SLURM_ARRAY_TASK_ID" hpc/cpu_submit
#! or read the task id directly in the Python script via: task_id = int(sys.argv[1])

cd $SLURM_SUBMIT_DIR

echo -e "Job ID: $SLURM_JOB_ID\nJob name: $SLURM_JOB_NAME\n"
echo "Time: `date`"
echo "Running on master node: `hostname`"
echo "Current directory: `pwd`"
echo -e "\nNodes allocated: num_tasks=$SLURM_NTASKS, num_nodes=$SLURM_JOB_NUM_NODES"
echo -e "\nExecuting command:\n$CMD\n\n==================\n"

eval $CMD
